{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import math\n",
    "import pprint\n",
    "import numpy as np\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import itertools  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine\n",
      "brazilian        467\n",
      "british          804\n",
      "cajun_creole    1546\n",
      "chinese         2673\n",
      "filipino         755\n",
      "french          2646\n",
      "greek           1175\n",
      "indian          3003\n",
      "irish            667\n",
      "italian         7838\n",
      "jamaican         526\n",
      "japanese        1423\n",
      "korean           830\n",
      "mexican         6438\n",
      "moroccan         821\n",
      "russian          489\n",
      "southern_us     4320\n",
      "spanish          989\n",
      "thai            1539\n",
      "vietnamese       825\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "#data = data.sample(frac=0.01,random_state=1)\n",
    "#print(data.head())\n",
    "#del data['id']\n",
    "print(data.groupby('cuisine').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSpaceModel(object):\n",
    "    \n",
    "    def tf_natural(self, tf, doc):\n",
    "    #return tf\n",
    "        return tf / sum(doc.values())\n",
    "    def idf_no(self, term):\n",
    "        return 1\n",
    "    def idf_idf(self, term):\n",
    "        return math.log10((len(self.docs.keys()) / (len(self.index[term]))) + 1 )\n",
    "    def norm_cosine(self,doc):\n",
    "        return 1 / (sum([tf**2 for tf in doc.values()]) + 1)\n",
    "    def norm_no(self, doc):\n",
    "        return 1  \n",
    "    def __init__(self, tf_func = 'natural', idf_func = 'idf', norm_func='none'):\n",
    "        self.tf_functions = {\n",
    "            'natural': self.tf_natural,\n",
    "            # 'logarithm': lambda tf, doc : (0 if tf==0 else 1 + math.log10(tf)),\n",
    "            # 'augmented': lambda tf, doc : (0.5 + ((0.5 * tf)/(self.find_max_tf(doc)))),\n",
    "            # 'boolean' : lambda tf, doc : (1 if tf > 0 else 0),\n",
    "            # 'log_ave': self.tf_log_ave\n",
    "\n",
    "        }\n",
    "        self.idf_functions = {\n",
    "            'no': self.idf_no,\n",
    "            'idf':self.idf_idf,\n",
    "            'prob_idf' : self.prod_idf\n",
    "        }\n",
    "        self.normailization_functions = {\n",
    "            'none' : self.norm_no,\n",
    "            'cosine' : self.norm_cosine,\n",
    "#             'pivoted_unique' : lambda \n",
    "        }\n",
    "        self.tf_func = self.tf_functions[tf_func]\n",
    "        self.idf_func = self.idf_functions[idf_func]\n",
    "        self.norm_func = self.normailization_functions[norm_func]\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.vocab_idf = {}\n",
    "        self.docs = {}\n",
    "        self.docs_char_length = {}\n",
    "        self.occurrance = {}\n",
    "        self.occurrance2 = {}\n",
    "        self.cdocs = {}\n",
    "        self.index = {}\n",
    "        def tf_log_ave(self, tf, doc):\n",
    "    #         print(tf)\n",
    "            return ( (1+math.log10((tf*sum(doc.values()))+1)) / (1 + math.log10(self.find_avg_tf(doc))))\n",
    "    def prod_idf(self, term):\n",
    "#         print('prob idf')\n",
    "#         print(math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))))\n",
    "        if 0 > math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))):\n",
    "            return 0\n",
    "        else:\n",
    "            return math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term])))    \n",
    "    \n",
    "    \n",
    "    def find_avg_tf(self, doc):\n",
    "        csum = 0\n",
    "        cnt = 0\n",
    "#         print(f'Doc sum : {sum(doc.values()) }')\n",
    "#         print(sum(doc.values()) / len(doc.keys()))\n",
    "        return (sum(doc.values()) / len(doc.keys()))\n",
    "    \n",
    "    def find_max_tf(self, doc):\n",
    "        max_tf = 0\n",
    "        max_term = None\n",
    "        for term, tf in doc.items():\n",
    "            if tf > max_tf:\n",
    "                max_tf = tf\n",
    "                max_term = term\n",
    "        return max_tf\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_doc(self, docId):\n",
    "        self.docs[docId] = dict.fromkeys(self.vocab, 0)\n",
    "        \n",
    "    def add_term(self, term, docId):\n",
    "        if term not in self.vocab.keys():\n",
    "            self.vocab[term] = 1\n",
    "            for Id, docList in self.docs.items():\n",
    "                self.docs[Id][term] = 0\n",
    "        else:\n",
    "            self.vocab[term]+=1\n",
    "        \n",
    "        if term in self.docs[docId].keys():\n",
    "            self.docs[docId][term] += 1\n",
    "        else:\n",
    "            self.docs[docId][term] = 1\n",
    "        \n",
    "                \n",
    "        if term in self.index.keys():\n",
    "            self.index[term].add(docId)\n",
    "        else:\n",
    "            self.index[term] = set()\n",
    "            self.index[term].add(docId)\n",
    "#         if docId not in self.occurrance.keys():\n",
    "#             self.occurrance[docId] = []\n",
    "#         self.occurrance[docId].append(position)\n",
    "    \n",
    "   \n",
    "\n",
    "    def calculate_tf_idf(self):\n",
    "        cdocs = {}\n",
    "        file = open('vectors.csv','w')\n",
    "        file.write(\"cuisine,\")\n",
    "        self.vocab_idf = dict.fromkeys(self.vocab, 0)\n",
    "        for term, term_cnt in self.vocab.items():\n",
    "            self.vocab_idf[term] = self.idf_func(term)\n",
    "            file.write(str(term))\n",
    "            file.write(',')\n",
    "        file.write('\\n')\n",
    "        \n",
    "        for docId,doc in self.docs.items():\n",
    "            #print(\"{},{}\".format(docId,doc))\n",
    "#             print(doc.values())\n",
    "            words_in_d = sum(doc.values())\n",
    "            tf = {}\n",
    "        \n",
    "            tf_idf = {}\n",
    "            file.write(str(data.query('id=={}'.format(docId))['cuisine'].iloc[0]))\n",
    "            file.write(',')\n",
    "            for term,term_cnt in doc.items():\n",
    "#                 tf[term] = term_cnt / words_in_d\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                tf[term] =self.tf_func(term_cnt, doc)\n",
    "                #tf_idf[term] = tf[term] * self.vocab_idf[term]\n",
    "                file.write(str(tf[term] * self.vocab_idf[term]))\n",
    "                file.write(',')\n",
    "            file.write('\\n')\n",
    "            #cdocs[docId] = {\n",
    "                \n",
    "             #   'tf_idf':tf_idf,\n",
    "            \n",
    "            #}\n",
    "        file.close()\n",
    "        return self.cdocs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "\n",
    "vector_space = VectorSpaceModel(tf_func='natural',idf_func='idf')\n",
    "printable = set(string.printable) \n",
    "raw_data = []\n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "lem = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    cuisine = data.iloc[i].cuisine\n",
    "    ingredient_string = data.iloc[i].ingredients\n",
    "    id = data.iloc[i].id\n",
    "    vector_space.create_doc(id)\n",
    "    ingredient=[]\n",
    "    for ing in ingredient_string:\n",
    "      ingredient.extend(ing.split(' '))\n",
    "    \n",
    "    \n",
    "    # print(ingredient)\n",
    "    # break\n",
    "    doc_set = set()\n",
    "    for word in ingredient:\n",
    "        terms = word.split(' ')\n",
    "        final_term = ''\n",
    "        for term in terms:\n",
    "            final_term+=lem.lemmatize(term)\n",
    "        word=final_term\n",
    "        vocab.add(word)\n",
    "    \n",
    "        doc_set.add(word)\n",
    "        # print(word)\n",
    "        vector_space.add_term(word, id)\n",
    "            \n",
    "    doc_contents.append(doc_set)\n",
    "    # print('*', end='')\n",
    "doc_term_tf_idf = vector_space.calculate_tf_idf()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "3377\n",
      "Total Number of Documents \n",
      "39774\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(vector_space.index.keys()))\n",
    "print('Total Number of Documents ')\n",
    "print(len(vector_space.docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
